{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5794207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import itertools\n",
    "from abc import ABC, abstractmethod\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sentence_transformers import SentenceTransformer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1217be7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseRecommender(ABC):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.train_df = None\n",
    "        \n",
    "    @abstractmethod\n",
    "    def fit(self, train_df):\n",
    "        \"\"\"Treina o modelo com os dados de treino.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def recommend(self, author_id, top_n=10):\n",
    "        \"\"\"Retorna uma lista de author_ids recomendados.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f903cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopologyRecommender(BaseRecommender):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"Topology (Graph Coauthor)\")\n",
    "        self.graph = defaultdict(set)\n",
    "        self.popular_authors = []\n",
    "        \n",
    "    def fit(self, train_df):\n",
    "        self.train_df = train_df\n",
    "        print(f\"[{self.name}] Construindo grafo...\")\n",
    "        \n",
    "        # Construção do Grafo\n",
    "        for _, group in train_df.groupby('work_id'):\n",
    "            authors = group['author_id'].tolist()\n",
    "            if len(authors) > 1:\n",
    "                for u, v in itertools.permutations(authors, 2):\n",
    "                    self.graph[u].add(v)\n",
    "        \n",
    "        # Cálculo de Popularidade (para fallback)\n",
    "        popularity_counter = Counter()\n",
    "        for author, neighbors in self.graph.items():\n",
    "            popularity_counter[author] = len(neighbors)\n",
    "        self.popular_authors = [auth for auth, _ in popularity_counter.most_common()]\n",
    "        print(f\"[{self.name}] Grafo construído com {len(self.graph)} autores.\")\n",
    "\n",
    "    def recommend(self, author_id, top_n=10):\n",
    "        recommendations = []\n",
    "        current_coauthors = self.graph.get(author_id, set())\n",
    "        \n",
    "        # Lógica de Amigos em Comum (2 hops)\n",
    "        if author_id in self.graph:\n",
    "            candidates = []\n",
    "            for neighbor in current_coauthors:\n",
    "                neighbors_of_neighbor = self.graph.get(neighbor, set())\n",
    "                for candidate in neighbors_of_neighbor:\n",
    "                    if candidate != author_id and candidate not in current_coauthors:\n",
    "                        candidates.append(candidate)\n",
    "            \n",
    "            recommendations = [c[0] for c in Counter(candidates).most_common(top_n)]\n",
    "        \n",
    "        # Fallback: Populares\n",
    "        if len(recommendations) < top_n:\n",
    "            for pop in self.popular_authors:\n",
    "                if pop != author_id and pop not in recommendations and pop not in current_coauthors:\n",
    "                    recommendations.append(pop)\n",
    "                    if len(recommendations) >= top_n:\n",
    "                        break\n",
    "                        \n",
    "        return recommendations[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e74db96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentSciBERTRecommender(BaseRecommender):\n",
    "    def __init__(self, model_name='allenai/scibert_scivocab_uncased'):\n",
    "        super().__init__(\"Content-Based (SciBERT)\")\n",
    "        self.model_name = model_name\n",
    "        self.encoder = None\n",
    "        self.author_embeddings = {} # {author_id: vector}\n",
    "        self.knn_model = None\n",
    "        self.author_ids_index = [] # Para mapear índice do KNN -> author_id\n",
    "        \n",
    "    def fit(self, train_df):\n",
    "        print(f\"[{self.name}] Carregando SciBERT e gerando embeddings...\")\n",
    "        self.encoder = SentenceTransformer(self.model_name)\n",
    "        \n",
    "        unique_works = train_df[['work_id', 'title', 'abstract']].drop_duplicates()\n",
    "        unique_works['text'] = unique_works['title'] + \". \" + unique_works['abstract'].fillna('')\n",
    "        \n",
    "        work_embeddings = self.encoder.encode(unique_works['text'].tolist(), show_progress_bar=True)\n",
    "        \n",
    "        work_id_to_emb = {wid: emb for wid, emb in zip(unique_works['work_id'], work_embeddings)}\n",
    "        \n",
    "        # O perfil do autor será a média dos vetores de seus trabalhos\n",
    "        print(f\"[{self.name}] Criando perfis de autores...\")\n",
    "        \n",
    "        author_groups = train_df.groupby('author_id')['work_id'].apply(list)\n",
    "        \n",
    "        author_vectors = []\n",
    "        self.author_ids_index = []\n",
    "        \n",
    "        for author_id, work_ids in author_groups.items():\n",
    "            vectors = [work_id_to_emb[wid] for wid in work_ids if wid in work_id_to_emb]\n",
    "            \n",
    "            if vectors:\n",
    "                mean_vector = np.mean(vectors, axis=0)\n",
    "                author_vectors.append(mean_vector)\n",
    "                self.author_ids_index.append(author_id)\n",
    "        \n",
    "        self.author_embeddings = np.array(author_vectors)\n",
    "        self.knn_model = NearestNeighbors(n_neighbors=50, metric='cosine', n_jobs=-1)\n",
    "        self.knn_model.fit(self.author_embeddings)\n",
    "        print(f\"[{self.name}] Treinamento concluído. {len(self.author_ids_index)} perfis criados.\")\n",
    "\n",
    "    def recommend(self, author_id, top_n=10):\n",
    "        try:\n",
    "            author_idx = self.author_ids_index.index(author_id)\n",
    "        except ValueError:\n",
    "            # TODO: Retornar autores populares\n",
    "            return []\n",
    "            \n",
    "        author_vector = self.author_embeddings[author_idx].reshape(1, -1)\n",
    "        \n",
    "        # Buscamos top_n + 1 porque o mais próximo é sempre ele mesmo\n",
    "        distances, indices = self.knn_model.kneighbors(author_vector, n_neighbors=top_n+1)\n",
    "        \n",
    "        recommendations = []\n",
    "        for idx in indices[0]:\n",
    "            rec_author = self.author_ids_index[idx]\n",
    "            if rec_author != author_id:\n",
    "                recommendations.append(rec_author)\n",
    "                \n",
    "        return recommendations[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48103f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models, test_ground_truth, train_graph_check, K_values=[5, 10]):\n",
    "    results = {}\n",
    "    \n",
    "    for model in models:\n",
    "        print(f\"\\nAvaliando modelo: {model.name}...\")\n",
    "        model_metrics = {k: {'precision': [], 'recall': []} for k in K_values}\n",
    "        \n",
    "        for author_id, actual_new_coauthors in test_ground_truth.items():\n",
    "            max_k = max(K_values)\n",
    "            recs = model.recommend(author_id, top_n=max_k)\n",
    "            \n",
    "            past_coauthors = train_graph_check.get(author_id, set())\n",
    "            recs = [r for r in recs if r not in past_coauthors]\n",
    "            \n",
    "            for k in K_values:\n",
    "                top_k_recs = recs[:k]\n",
    "                hits = len(set(top_k_recs) & actual_new_coauthors)\n",
    "                \n",
    "                p = hits / k if k > 0 else 0\n",
    "                r = hits / len(actual_new_coauthors) if len(actual_new_coauthors) > 0 else 0\n",
    "                \n",
    "                model_metrics[k]['precision'].append(p)\n",
    "                model_metrics[k]['recall'].append(r)\n",
    "        \n",
    "        # Média final\n",
    "        results[model.name] = {}\n",
    "        for k in K_values:\n",
    "            avg_p = np.mean(model_metrics[k]['precision'])\n",
    "            avg_r = np.mean(model_metrics[k]['recall'])\n",
    "            f1 = 2 * (avg_p * avg_r) / (avg_p + avg_r) if (avg_p + avg_r) > 0 else 0\n",
    "            \n",
    "            results[model.name][k] = {'P': avg_p, 'R': avg_r, 'F1': f1}\n",
    "            print(f\"  K={k}: Precision={avg_p:.4f}, Recall={avg_r:.4f}, F1={f1:.4f}\")\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d01cf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_df = pd.read_csv('database/authorships.csv')\n",
    "works_df = pd.read_csv('database/works.csv')\n",
    "\n",
    "merged_df = authors_df.merge(\n",
    "    works_df[['id', 'publication_date', 'title', 'abstract', 'language']], \n",
    "    left_on='work_id', right_on='id'\n",
    ")\n",
    "merged_df['publication_date'] = pd.to_datetime(merged_df['publication_date'], errors='coerce')\n",
    "merged_df = merged_df.dropna(subset=['publication_date', 'author_id', 'title', 'abstract', 'language']).drop(columns=['id'])\n",
    "merged_df = merged_df[merged_df['language'] == 'en']\n",
    "\n",
    "unique_works = merged_df[['work_id', 'publication_date']].drop_duplicates().sort_values('publication_date')\n",
    "split_idx = int(len(unique_works) * 0.8)\n",
    "\n",
    "train_work_ids = set(unique_works.iloc[:split_idx]['work_id'])\n",
    "test_work_ids = set(unique_works.iloc[split_idx:]['work_id'])\n",
    "\n",
    "train_df = merged_df[merged_df['work_id'].isin(train_work_ids)]\n",
    "test_df = merged_df[merged_df['work_id'].isin(test_work_ids)]\n",
    "\n",
    "def build_graph(df):\n",
    "    graph = defaultdict(set)\n",
    "    for _, group in df.groupby('work_id'):\n",
    "        authors = group['author_id'].tolist()\n",
    "        \n",
    "        if len(authors) > 1:\n",
    "            for u, v in itertools.permutations(authors, 2):\n",
    "                graph[u].add(v)\n",
    "\n",
    "    return graph\n",
    "\n",
    "train_graph = build_graph(train_df)\n",
    "test_graph_raw = build_graph(test_df)\n",
    "\n",
    "test_ground_truth = defaultdict(set)\n",
    "\n",
    "for author, coauthors in test_graph_raw.items():\n",
    "    # Pega quem o autor colaborou no futuro\n",
    "    future_coauthors = coauthors\n",
    "    \n",
    "    # Remove quem ele já conhecia no passado (não é predição nova)\n",
    "    past_coauthors = train_graph.get(author, set())\n",
    "    new_links = future_coauthors - past_coauthors\n",
    "    \n",
    "    if new_links:\n",
    "        test_ground_truth[author] = new_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1b084bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Topology (Graph Coauthor)] Construindo grafo...\n",
      "[Topology (Graph Coauthor)] Grafo construído com 5320 autores.\n",
      "\n",
      "Avaliando modelo: Topology (Graph Coauthor)...\n",
      "  K=5: Precision=0.0606, Recall=0.0295, F1=0.0397\n",
      "  K=10: Precision=0.0426, Recall=0.0419, F1=0.0422\n",
      "  K=20: Precision=0.0290, Recall=0.0583, F1=0.0387\n",
      "  K=50: Precision=0.0177, Recall=0.0835, F1=0.0292\n"
     ]
    }
   ],
   "source": [
    "topo_model = TopologyRecommender()\n",
    "\n",
    "models = [topo_model]\n",
    "\n",
    "for model in models:\n",
    "    model.fit(train_df) \n",
    "    \n",
    "metrics = evaluate_models(models, test_ground_truth, train_graph, K_values=[5, 10, 20, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e59c0539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Content-Based (SciBERT)] Carregando SciBERT e gerando embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name allenai/scibert_scivocab_uncased. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7012b2b750341aca310de3fe27594e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: allenai/scibert_scivocab_uncased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "cls.predictions.decoder.bias               | UNEXPECTED |  | \n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "cls.predictions.decoder.weight             | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4839c293555409e8749f6d351471d9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Content-Based (SciBERT)] Criando perfis de autores...\n",
      "[Content-Based (SciBERT)] Treinamento concluído. 5320 perfis criados.\n",
      "\n",
      "Avaliando modelo: Content-Based (SciBERT)...\n",
      "  K=5: Precision=0.0070, Recall=0.0060, F1=0.0065\n",
      "  K=10: Precision=0.0052, Recall=0.0077, F1=0.0062\n",
      "  K=20: Precision=0.0036, Recall=0.0100, F1=0.0053\n",
      "  K=50: Precision=0.0019, Recall=0.0128, F1=0.0033\n"
     ]
    }
   ],
   "source": [
    "content_model = ContentSciBERTRecommender(model_name='allenai/scibert_scivocab_uncased')\n",
    "\n",
    "models = [content_model]\n",
    "\n",
    "for model in models:\n",
    "    model.fit(train_df) \n",
    "    \n",
    "metrics = evaluate_models(models, test_ground_truth, train_graph, K_values=[5, 10, 20, 50])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

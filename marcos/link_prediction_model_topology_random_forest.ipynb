{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c991206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from collections import defaultdict, Counter\n",
    "from abc import ABC, abstractmethod\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sentence_transformers import SentenceTransformer \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore', message='.*sklearn.utils.parallel.delayed.*', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9a214cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseRecommender(ABC):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.train_df = None\n",
    "        \n",
    "    @abstractmethod\n",
    "    def fit(self, train_df):\n",
    "        \"\"\"Treina o modelo com os dados de treino.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def recommend(self, author_id, top_n=10):\n",
    "        \"\"\"Retorna uma lista de author_ids recomendados.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dff86c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import math\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "class HybridCoauthorRecommender(BaseRecommender):\n",
    "    def __init__(self, candidate_pool_size=100, n_estimators=100):\n",
    "        super().__init__(\"Hybrid (Graph + RandomForest)\")\n",
    "        self.graph = defaultdict(set)\n",
    "        self.popular_authors = []\n",
    "        self.rf_model = RandomForestClassifier(n_estimators=n_estimators, random_state=42)\n",
    "        self.candidate_pool_size = candidate_pool_size\n",
    "        \n",
    "    def _build_graph(self, train_df):\n",
    "        \"\"\"Constrói o grafo bidirecional de coautorias.\"\"\"\n",
    "        for _, group in train_df.groupby(\"work_id\"):\n",
    "            authors = group[\"author_id\"].tolist()\n",
    "            if len(authors) > 1:\n",
    "                for a, b in itertools.combinations(authors, 2):\n",
    "                    self.graph[a].add(b)\n",
    "                    self.graph[b].add(a) \n",
    "                    \n",
    "        # Popularidade para fallback\n",
    "        popularity_counter = Counter({a: len(neighbors) for a, neighbors in self.graph.items()})\n",
    "        self.popular_authors = [auth for auth, _ in popularity_counter.most_common()]\n",
    "\n",
    "    def _extract_features(self, u, v):\n",
    "        \"\"\"Calcula features topológicas entre dois autores.\"\"\"\n",
    "        neighbors_u = self.graph.get(u, set())\n",
    "        neighbors_v = self.graph.get(v, set())\n",
    "        \n",
    "        common_neighbors = neighbors_u & neighbors_v\n",
    "        union_neighbors = neighbors_u | neighbors_v\n",
    "        \n",
    "        # Feature 1: Common Neighbors (Contagem bruta)\n",
    "        cn_score = len(common_neighbors)\n",
    "        \n",
    "        # Feature 2: Jaccard Similarity (Proporção)\n",
    "        jaccard_score = cn_score / len(union_neighbors) if len(union_neighbors) > 0 else 0\n",
    "        \n",
    "        # Feature 3: Adamic-Adar Index (Peso aos vizinhos menos populares)\n",
    "        aa_score = 0\n",
    "        for z in common_neighbors:\n",
    "            degree_z = len(self.graph.get(z, set()))\n",
    "            if degree_z > 1: # Evitar divisão por zero ou log(1)\n",
    "                aa_score += 1 / math.log(degree_z)\n",
    "                \n",
    "        return [cn_score, jaccard_score, aa_score]\n",
    "\n",
    "    def _generate_training_data(self):\n",
    "        \"\"\"Gera o dataset para treinar a Random Forest (Amostras Positivas e Negativas).\"\"\"\n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        # 1. Amostras Positivas (y=1): Arestas que realmente existem no grafo\n",
    "        positive_edges = set()\n",
    "        for u, neighbors in self.graph.items():\n",
    "            for v in neighbors:\n",
    "                if u < v: # Evitar duplicatas (u,v) e (v,u)\n",
    "                    positive_edges.add((u, v))\n",
    "        \n",
    "        positive_edges = list(positive_edges)\n",
    "        \n",
    "        # Para evitar estourar a memória, limitamos o número de amostras\n",
    "        # (Ajuste esse limite dependendo do tamanho da sua RAM)\n",
    "        max_samples = min(len(positive_edges), 100000) \n",
    "        sampled_positives = random.sample(positive_edges, max_samples)\n",
    "        \n",
    "        for u, v in sampled_positives:\n",
    "            X.append(self._extract_features(u, v))\n",
    "            y.append(1)\n",
    "            \n",
    "        # 2. Amostras Negativas (y=0): \"Hard Negatives\" (Amigos em comum, mas sem coautoria)\n",
    "        # Isso ensina o modelo a diferenciar um falso positivo de uma coautoria real\n",
    "        print(f\"[{self.name}] Gerando amostras negativas...\")\n",
    "        negatives_generated = 0\n",
    "        \n",
    "        while negatives_generated < max_samples:\n",
    "            u = random.choice(list(self.graph.keys()))\n",
    "            neighbors_u = self.graph.get(u, set())\n",
    "            \n",
    "            if not neighbors_u:\n",
    "                continue\n",
    "                \n",
    "            # Pega um vizinho do vizinho (2-hop)\n",
    "            neighbor = random.choice(list(neighbors_u))\n",
    "            neighbors_of_neighbor = self.graph.get(neighbor, set())\n",
    "            \n",
    "            if not neighbors_of_neighbor:\n",
    "                continue\n",
    "                \n",
    "            v = random.choice(list(neighbors_of_neighbor))\n",
    "            \n",
    "            # Condição crucial: u e v não podem ser a mesma pessoa e não podem ser coautores diretos\n",
    "            if u != v and v not in neighbors_u:\n",
    "                X.append(self._extract_features(u, v))\n",
    "                y.append(0)\n",
    "                negatives_generated += 1\n",
    "                \n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    def fit(self, train_df):\n",
    "        print(f\"[{self.name}] Passo 1: Construindo grafo base...\")\n",
    "        self._build_graph(train_df)\n",
    "        \n",
    "        print(f\"[{self.name}] Passo 2: Extraindo features e amostras (Positivas/Negativas)...\")\n",
    "        X_train, y_train = self._generate_training_data()\n",
    "        \n",
    "        print(f\"[{self.name}] Passo 3: Treinando Random Forest ({len(y_train)} amostras)...\")\n",
    "        self.rf_model.fit(X_train, y_train)\n",
    "        print(f\"[{self.name}] Modelo treinado com sucesso!\")\n",
    "\n",
    "    def recommend(self, author_id, top_n=10):\n",
    "        # ---------------------------------------------------------\n",
    "        # ESTÁGIO 1: Candidate Generation (Filtro Rápido via Grafo)\n",
    "        # ---------------------------------------------------------\n",
    "        current_coauthors = self.graph.get(author_id, set())\n",
    "        candidate_scores = Counter()\n",
    "        \n",
    "        # Busca em 2-hops\n",
    "        for neighbor in current_coauthors:\n",
    "            for candidate in self.graph.get(neighbor, set()):\n",
    "                if candidate != author_id and candidate not in current_coauthors:\n",
    "                    candidate_scores[candidate] += 1\n",
    "                    \n",
    "        # Pega os Top-K candidatos mais promissores (pool)\n",
    "        top_candidates = [c for c, _ in candidate_scores.most_common(self.candidate_pool_size)]\n",
    "        \n",
    "        # ---------------------------------------------------------\n",
    "        # ESTÁGIO 2: Ranking (Re-ranqueamento via Random Forest)\n",
    "        # ---------------------------------------------------------\n",
    "        if top_candidates:\n",
    "            # Extrair features só para os candidatos filtrados (muito rápido)\n",
    "            X_candidates = [self._extract_features(author_id, candidate) for candidate in top_candidates]\n",
    "            \n",
    "            # Prever a probabilidade de ser uma coautoria (y=1)\n",
    "            # predict_proba retorna [prob_0, prob_1], queremos a prob_1\n",
    "            probabilities = self.rf_model.predict_proba(X_candidates)[:, 1]\n",
    "            \n",
    "            # Combinar candidatos com suas probabilidades e ordenar\n",
    "            scored_candidates = list(zip(top_candidates, probabilities))\n",
    "            scored_candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            recommendations = [candidate for candidate, prob in scored_candidates]\n",
    "        else:\n",
    "            recommendations = []\n",
    "            \n",
    "        # Fallback: Completar com populares se não houver candidatos suficientes\n",
    "        if len(recommendations) < top_n:\n",
    "            for pop in self.popular_authors:\n",
    "                if pop != author_id and pop not in recommendations and pop not in current_coauthors:\n",
    "                    recommendations.append(pop)\n",
    "                    if len(recommendations) >= top_n:\n",
    "                        break\n",
    "                        \n",
    "        return recommendations[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cd1af55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import itertools\n",
    "\n",
    "class TopologyRecommender(BaseRecommender):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"Topology (Graph Coauthor)\")\n",
    "        self.graph = defaultdict(set)\n",
    "        self.popular_authors = []\n",
    "        \n",
    "    def fit(self, train_df):\n",
    "        self.train_df = train_df\n",
    "        print(f\"[{self.name}] Construindo grafo...\")\n",
    "        \n",
    "        # Construção do Grafo\n",
    "        for _, group in train_df.groupby('work_id'):\n",
    "            authors = group['author_id'].tolist()\n",
    "            if len(authors) > 1:\n",
    "                for u, v in itertools.combinations(authors, 2):\n",
    "                    self.graph[u].add(v)\n",
    "        \n",
    "        # Cálculo de Popularidade (para fallback)\n",
    "        popularity_counter = Counter()\n",
    "        for author, neighbors in self.graph.items():\n",
    "            popularity_counter[author] = len(neighbors)\n",
    "        self.popular_authors = [auth for auth, _ in popularity_counter.most_common()]\n",
    "        print(f\"[{self.name}] Grafo construído com {len(self.graph)} autores.\")\n",
    "\n",
    "    def recommend(self, author_id, top_n=10):\n",
    "        recommendations = []\n",
    "        current_coauthors = self.graph.get(author_id, set())\n",
    "        \n",
    "        # Determinar se top_n é negativo (modo mínimo)\n",
    "        is_minimum_mode = top_n < 0\n",
    "        target_n = abs(top_n) if is_minimum_mode else top_n\n",
    "        \n",
    "        # Lógica de Amigos em Comum (2 hops) - Sem otimização de memória\n",
    "        if author_id in self.graph:\n",
    "            candidates = []\n",
    "            for neighbor in current_coauthors:\n",
    "                neighbors_of_neighbor = self.graph.get(neighbor, set())\n",
    "                for candidate in neighbors_of_neighbor:\n",
    "                    if candidate != author_id and candidate not in current_coauthors:\n",
    "                        candidates.append(candidate)\n",
    "            \n",
    "            if is_minimum_mode:\n",
    "                recommendations = [c[0] for c in Counter(candidates).most_common()]\n",
    "            else:\n",
    "                recommendations = [c[0] for c in Counter(candidates).most_common(top_n)]\n",
    "        \n",
    "        # Fallback: Populares\n",
    "        if len(recommendations) < target_n:\n",
    "            for pop in self.popular_authors:\n",
    "                if pop != author_id and pop not in recommendations and pop not in current_coauthors:\n",
    "                    recommendations.append(pop)\n",
    "                    if len(recommendations) >= target_n:\n",
    "                        break\n",
    "        \n",
    "        if not is_minimum_mode:\n",
    "            return recommendations[:top_n]\n",
    "        else:\n",
    "            return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a45c705",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:\\\\Users\\\\BSBCo\\\\dev\\\\link-prediction\\\\marcos\\\\database_50k/authorships.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m root_path = os.path.abspath(os.getcwd())\n\u001b[32m      4\u001b[39m database_path = os.path.join(root_path, \u001b[33m\"\u001b[39m\u001b[33mdatabase_50k\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m authors_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdatabase_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/authorships.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m works_df = pd.read_csv(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatabase_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/works.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      8\u001b[39m merged_df = authors_df.merge(\n\u001b[32m      9\u001b[39m     works_df[[\u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mpublication_date\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mabstract\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mlanguage\u001b[39m\u001b[33m'\u001b[39m]], \n\u001b[32m     10\u001b[39m     left_on=\u001b[33m'\u001b[39m\u001b[33mwork_id\u001b[39m\u001b[33m'\u001b[39m, right_on=\u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     11\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\BSBCo\\dev\\link-prediction\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:873\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, skip_blank_lines, parse_dates, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m    861\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m    862\u001b[39m     dialect,\n\u001b[32m    863\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    869\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m    870\u001b[39m )\n\u001b[32m    871\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\BSBCo\\dev\\link-prediction\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:300\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    297\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    299\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\BSBCo\\dev\\link-prediction\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1645\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1642\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1644\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1645\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\BSBCo\\dev\\link-prediction\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1904\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1902\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1903\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1904\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1905\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1906\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1907\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1908\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1909\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1910\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1911\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1912\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1913\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1914\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1915\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\BSBCo\\dev\\link-prediction\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:926\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    921\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    922\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    923\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    924\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    935\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'c:\\\\Users\\\\BSBCo\\\\dev\\\\link-prediction\\\\marcos\\\\database_50k/authorships.csv'"
     ]
    }
   ],
   "source": [
    "database_path = '../database_50k'\n",
    "authors_df = pd.read_csv(f'{database_path}/authorships.csv')\n",
    "works_df = pd.read_csv(f'{database_path}/works.csv')\n",
    "\n",
    "merged_df = authors_df.merge(\n",
    "    works_df[['id', 'publication_date', 'title', 'abstract', 'language']], \n",
    "    left_on='work_id', right_on='id'\n",
    ")\n",
    "merged_df['publication_date'] = pd.to_datetime(merged_df['publication_date'], errors='coerce')\n",
    "merged_df = merged_df.dropna(subset=['publication_date', 'author_id', 'title', 'abstract', 'language']).drop(columns=['id'])\n",
    "merged_df = merged_df[merged_df['language'] == 'en']\n",
    "\n",
    "# Divisão Temporal: 80% Treino, 20% Teste\n",
    "print(\"Dividindo dados temporalmente (80/20)...\")\n",
    "unique_works = merged_df[['work_id', 'publication_date']].drop_duplicates().sort_values('publication_date')\n",
    "total_works = len(unique_works)\n",
    "\n",
    "split_train = int(total_works * 0.8)\n",
    "\n",
    "train_work_ids = set(unique_works.iloc[:split_train]['work_id'])\n",
    "test_work_ids = set(unique_works.iloc[split_train:]['work_id'])\n",
    "\n",
    "train_df = merged_df[merged_df['work_id'].isin(train_work_ids)]\n",
    "test_df = merged_df[merged_df['work_id'].isin(test_work_ids)]\n",
    "\n",
    "print(f\"\\n=== RESUMO DO SPLIT ===\")\n",
    "print(f\"Trabalhos no Treino: {len(train_work_ids)}\")\n",
    "print(f\"Trabalhos no Teste: {len(test_work_ids)}\")\n",
    "print(f\"Total de Autores no Treino: {len(set(train_df['author_id']))}\")\n",
    "print(f\"Total de Autores no Teste: {len(set(test_df['author_id']))}\")\n",
    "\n",
    "def build_graph(df):\n",
    "    graph = defaultdict(set)\n",
    "    for _, group in df.groupby('work_id'):\n",
    "        authors = group['author_id'].tolist()\n",
    "        \n",
    "        if len(authors) > 1:\n",
    "            for u, v in itertools.combinations(authors, 2):\n",
    "                graph[u].add(v)\n",
    "\n",
    "    return graph\n",
    "\n",
    "# Construir grafos para avaliação\n",
    "train_graph = build_graph(train_df)\n",
    "test_graph_raw = build_graph(test_df)\n",
    "\n",
    "# Ground truth para teste: novos links que apareceram no teste mas não no treino\n",
    "test_ground_truth = defaultdict(set)\n",
    "for author, coauthors in test_graph_raw.items():\n",
    "    future_coauthors = coauthors\n",
    "    past_coauthors = train_graph.get(author, set())\n",
    "    new_links = future_coauthors - past_coauthors\n",
    "    \n",
    "    if new_links:\n",
    "        test_ground_truth[author] = new_links\n",
    "\n",
    "print(f\"\\nAutores alvo no teste (com novos links): {len(test_ground_truth)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9846681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ndcg(recommended_list, relevant_set, k):\n",
    "    \"\"\"\n",
    "    Calcula o NDCG@k (Normalized Discounted Cumulative Gain).\n",
    "\n",
    "    Args:\n",
    "        recommended_list: Lista de itens recomendados (ordenados)\n",
    "        relevant_set: Conjunto de itens relevantes (ground truth)\n",
    "        k: Número de itens a considerar (top-k)\n",
    "\n",
    "    Returns:\n",
    "        NDCG@k score (0.0 a 1.0)\n",
    "    \"\"\"\n",
    "    if len(relevant_set) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # DCG: Discounted Cumulative Gain\n",
    "    dcg = 0.0\n",
    "    for i, item in enumerate(recommended_list[:k], 1):\n",
    "        if item in relevant_set:\n",
    "            dcg += 1.0 / np.log2(i + 1)\n",
    "\n",
    "    # IDCG: Ideal DCG (ordenando os relevantes primeiro)\n",
    "    idcg = 0.0\n",
    "    num_relevant = min(len(relevant_set), k)\n",
    "    for i in range(1, num_relevant + 1):\n",
    "        idcg += 1.0 / np.log2(i + 1)\n",
    "\n",
    "    # NDCG = DCG / IDCG\n",
    "    if idcg == 0:\n",
    "        return 0.0\n",
    "    return dcg / idcg\n",
    "\n",
    "\n",
    "def calculate_mrr_at_k(recommended_list, relevant_set, k):\n",
    "    \"\"\"\n",
    "    Calcula o MRR@k (Mean Reciprocal Rank at k).\n",
    "\n",
    "    O MRR@k só considera o primeiro item relevante se ele aparecer dentro das\n",
    "    primeiras k posições. Se o primeiro relevante aparecer após a posição k,\n",
    "    o MRR@k é 0.\n",
    "\n",
    "    Args:\n",
    "        recommended_list: Lista de itens recomendados (ordenados)\n",
    "        relevant_set: Conjunto de itens relevantes (ground truth)\n",
    "        k: Número máximo de posições a considerar (top-k)\n",
    "\n",
    "    Returns:\n",
    "        Reciprocal Rank@k (1/rank do primeiro relevante dentro de k, ou 0 se nenhum relevante)\n",
    "    \"\"\"\n",
    "    if len(relevant_set) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Considera apenas as primeiras k posições\n",
    "    for rank, item in enumerate(recommended_list[:k], 1):\n",
    "        if item in relevant_set:\n",
    "            return 1.0 / rank\n",
    "\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def evaluate_models(models, test_ground_truth, train_graph_check, K_values=[5, 10]):\n",
    "    results = {}\n",
    "\n",
    "    # 1. Definir o maior K necessário para a avaliação\n",
    "    max_k_eval = max(K_values)\n",
    "\n",
    "    # 2. Definir um \"Buffer\" de busca.\n",
    "    # Pedimos 2x ou 3x mais itens para garantir que, após remover\n",
    "    # os coautores antigos, ainda sobrem itens suficientes para encher o max_k_eval.\n",
    "    search_limit = max_k_eval * 3\n",
    "\n",
    "    for model in models:\n",
    "        print(f\"\\nAvaliando modelo: {model.name}...\")\n",
    "        model_metrics = {\n",
    "            k: {\"precision\": [], \"recall\": [], \"ndcg\": [], \"mrr\": []} for k in K_values\n",
    "        }\n",
    "\n",
    "        # Adicionar barra de progresso com tqdm\n",
    "        total_authors = len(test_ground_truth)\n",
    "        for author_id, actual_new_coauthors in tqdm(\n",
    "            test_ground_truth.items(),\n",
    "            desc=f\"  {model.name}\",\n",
    "            total=total_authors,\n",
    "            unit=\"autor\",\n",
    "        ):\n",
    "            # SOLICITAÇÃO EXPANDIDA: Pede mais itens do que o necessário\n",
    "            recs = model.recommend(author_id, top_n=search_limit)\n",
    "\n",
    "            # FILTRAGEM: Remove coautores que já existem no treino\n",
    "            past_coauthors = train_graph_check.get(author_id, set())\n",
    "            valid_recs = [r for r in recs if r not in past_coauthors]\n",
    "\n",
    "            # CORTE FINAL: Garante que a lista tenha no máximo o tamanho do maior K avaliado\n",
    "            # Isso garante consistência: estamos avaliando as melhores \"valid_recs\" disponíveis\n",
    "            valid_recs = valid_recs[:max_k_eval]\n",
    "\n",
    "            for k in K_values:\n",
    "                # O top_k aqui é seguro, pois valid_recs já está limpo\n",
    "                top_k_recs = valid_recs[:k]\n",
    "\n",
    "                hits = len(set(top_k_recs) & actual_new_coauthors)\n",
    "\n",
    "                # Métricas padrão\n",
    "                p = hits / k if k > 0 else 0\n",
    "                r = (\n",
    "                    hits / len(actual_new_coauthors)\n",
    "                    if len(actual_new_coauthors) > 0\n",
    "                    else 0\n",
    "                )\n",
    "                ndcg = calculate_ndcg(top_k_recs, actual_new_coauthors, k)\n",
    "                mrr = calculate_mrr_at_k(\n",
    "                    valid_recs, actual_new_coauthors, k\n",
    "                )  # MRR usa a lista até K implícito\n",
    "\n",
    "                model_metrics[k][\"precision\"].append(p)\n",
    "                model_metrics[k][\"recall\"].append(r)\n",
    "                model_metrics[k][\"ndcg\"].append(ndcg)\n",
    "                model_metrics[k][\"mrr\"].append(mrr)\n",
    "\n",
    "        # Consolidação dos resultados (Média)\n",
    "        results[model.name] = {}\n",
    "        for k in K_values:\n",
    "            avg_p = np.mean(model_metrics[k][\"precision\"])\n",
    "            avg_r = np.mean(model_metrics[k][\"recall\"])\n",
    "            avg_ndcg = np.mean(model_metrics[k][\"ndcg\"])\n",
    "            avg_mrr = np.mean(model_metrics[k][\"mrr\"])\n",
    "            f1 = 2 * (avg_p * avg_r) / (avg_p + avg_r) if (avg_p + avg_r) > 0 else 0\n",
    "\n",
    "            results[model.name][k] = {\n",
    "                \"P\": avg_p,\n",
    "                \"R\": avg_r,\n",
    "                \"F1\": f1,\n",
    "                \"NDCG\": avg_ndcg,\n",
    "                \"MRR\": avg_mrr,\n",
    "            }\n",
    "            print(\n",
    "                f\"  K={k}: Precision={avg_p:.4f}, Recall={avg_r:.4f}, F1={f1:.4f}, NDCG={avg_ndcg:.4f}, MRR@k={avg_mrr:.4f}\"\n",
    "            )\n",
    "\n",
    "    return results\n",
    "\n",
    "def plot_model_comparison(results, figsize=(25, 5)):\n",
    "    model_names = list(results.keys())\n",
    "    k_values = sorted([k for k in results[model_names[0]].keys() if isinstance(k, int)])\n",
    "    \n",
    "    # Preparar dados para cada métrica\n",
    "    metrics_data = {\n",
    "        'Precision': {model: [results[model][k]['P'] for k in k_values] for model in model_names},\n",
    "        'Recall': {model: [results[model][k]['R'] for k in k_values] for model in model_names},\n",
    "        'F1-Score': {model: [results[model][k]['F1'] for k in k_values] for model in model_names},\n",
    "        'NDCG': {model: [results[model][k]['NDCG'] for k in k_values] for model in model_names},\n",
    "        'MRR@k': {model: [results[model][k]['MRR'] for k in k_values] for model in model_names}\n",
    "    }\n",
    "    \n",
    "    # Criar figura com 5 subplots para métricas dependentes de K\n",
    "    fig, axes = plt.subplots(1, 5, figsize=figsize)\n",
    "    fig.suptitle('Comparação de Modelos de Link Prediction', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Cores e estilos para cada modelo\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "    markers = ['o', 's', '^', 'D', 'v']\n",
    "    \n",
    "    # Plotar cada métrica dependente de K\n",
    "    for idx, (metric_name, data) in enumerate(metrics_data.items()):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        for i, model in enumerate(model_names):\n",
    "            ax.plot(\n",
    "                k_values, \n",
    "                data[model], \n",
    "                marker=markers[i % len(markers)],\n",
    "                label=model,\n",
    "                color=colors[i % len(colors)],\n",
    "                linewidth=2,\n",
    "                markersize=8\n",
    "            )\n",
    "        \n",
    "        ax.set_xlabel('K (Top-K)', fontsize=11)\n",
    "        ax.set_ylabel(metric_name, fontsize=11)\n",
    "        ax.set_title(f'{metric_name} por K', fontsize=12, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3, linestyle='--')\n",
    "        ax.legend(fontsize=9)\n",
    "        ax.set_xticks(k_values)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878e3105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TREINANDO MODELOS\n",
      "============================================================\n",
      "[Topology (Graph Coauthor)] Construindo grafo...\n",
      "[Topology (Graph Coauthor)] Grafo construído com 22463 autores.\n",
      "[Hybrid (Graph + RandomForest)] Passo 1: Construindo grafo base...\n",
      "[Hybrid (Graph + RandomForest)] Passo 2: Extraindo features e amostras (Positivas/Negativas)...\n",
      "[Hybrid (Graph + RandomForest)] Gerando amostras negativas...\n",
      "[Hybrid (Graph + RandomForest)] Passo 3: Treinando Random Forest (200000 amostras)...\n",
      "[Hybrid (Graph + RandomForest)] Modelo treinado com sucesso!\n",
      "\n",
      "============================================================\n",
      "AVALIANDO MODELOS NO TESTE\n",
      "============================================================\n",
      "\n",
      "Avaliando modelo: Topology (Graph Coauthor)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Topology (Graph Coauthor): 100%|██████████| 10007/10007 [00:26<00:00, 376.21autor/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  K=5: Precision=0.0116, Recall=0.0136, F1=0.0125, NDCG=0.0154, MRR@k=0.0271\n",
      "  K=10: Precision=0.0089, Recall=0.0195, F1=0.0122, NDCG=0.0169, MRR@k=0.0304\n",
      "  K=20: Precision=0.0075, Recall=0.0341, F1=0.0123, NDCG=0.0211, MRR@k=0.0332\n",
      "  K=50: Precision=0.0056, Recall=0.0622, F1=0.0102, NDCG=0.0289, MRR@k=0.0366\n",
      "  K=100: Precision=0.0036, Recall=0.0819, F1=0.0070, NDCG=0.0334, MRR@k=0.0374\n",
      "  K=200: Precision=0.0026, Recall=0.1146, F1=0.0050, NDCG=0.0401, MRR@k=0.0382\n",
      "\n",
      "Avaliando modelo: Hybrid (Graph + RandomForest)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Hybrid (Graph + RandomForest): 100%|██████████| 10007/10007 [01:00<00:00, 164.92autor/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  K=5: Precision=0.0109, Recall=0.0129, F1=0.0118, NDCG=0.0134, MRR@k=0.0225\n",
      "  K=10: Precision=0.0103, Recall=0.0242, F1=0.0144, NDCG=0.0173, MRR@k=0.0278\n",
      "  K=20: Precision=0.0108, Recall=0.0516, F1=0.0178, NDCG=0.0264, MRR@k=0.0344\n",
      "  K=50: Precision=0.0080, Recall=0.0898, F1=0.0147, NDCG=0.0374, MRR@k=0.0390\n",
      "  K=100: Precision=0.0051, Recall=0.1180, F1=0.0098, NDCG=0.0439, MRR@k=0.0401\n",
      "  K=200: Precision=0.0031, Recall=0.1440, F1=0.0060, NDCG=0.0490, MRR@k=0.0407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Treinar modelos\n",
    "print(\"=\"*60)\n",
    "print(\"TREINANDO MODELOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "topo_model = TopologyRecommender()\n",
    "hybrid_model = HybridCoauthorRecommender()\n",
    "\n",
    "models = [\n",
    "    topo_model,\n",
    "    hybrid_model\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    model.fit(train_df)\n",
    "    \n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AVALIANDO MODELOS NO TESTE\")\n",
    "print(\"=\"*60)\n",
    "    \n",
    "metrics = evaluate_models(models, test_ground_truth, train_graph, K_values=[5, 10, 20, 50, 100, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700170d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_comparison(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b046ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construindo dicionário de contagem de coautorias...\n",
      "Construindo grafo de coautores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando trabalhos: 100%|██████████| 19916/19916 [00:03<00:00, 5853.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de pares que são coautores: 217,493\n",
      "\n",
      "Criando dataset com pares de coautores e calculando autores em comum...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando pares: 100%|██████████| 217493/217493 [00:01<00:00, 185534.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset criado! Total de linhas: 217,493\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_a</th>\n",
       "      <th>author_b</th>\n",
       "      <th>count_coauthors</th>\n",
       "      <th>common_coauthors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://openalex.org/A5076219710</td>\n",
       "      <td>https://openalex.org/A5103241656</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://openalex.org/A5035579644</td>\n",
       "      <td>https://openalex.org/A5103241656</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://openalex.org/A5069881493</td>\n",
       "      <td>https://openalex.org/A5103241656</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://openalex.org/A5006873642</td>\n",
       "      <td>https://openalex.org/A5103241656</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://openalex.org/A5103241656</td>\n",
       "      <td>https://openalex.org/A5110165399</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://openalex.org/A5035579644</td>\n",
       "      <td>https://openalex.org/A5076219710</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://openalex.org/A5069881493</td>\n",
       "      <td>https://openalex.org/A5076219710</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://openalex.org/A5006873642</td>\n",
       "      <td>https://openalex.org/A5076219710</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://openalex.org/A5076219710</td>\n",
       "      <td>https://openalex.org/A5110165399</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://openalex.org/A5035579644</td>\n",
       "      <td>https://openalex.org/A5069881493</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           author_a                          author_b  \\\n",
       "0  https://openalex.org/A5076219710  https://openalex.org/A5103241656   \n",
       "1  https://openalex.org/A5035579644  https://openalex.org/A5103241656   \n",
       "2  https://openalex.org/A5069881493  https://openalex.org/A5103241656   \n",
       "3  https://openalex.org/A5006873642  https://openalex.org/A5103241656   \n",
       "4  https://openalex.org/A5103241656  https://openalex.org/A5110165399   \n",
       "5  https://openalex.org/A5035579644  https://openalex.org/A5076219710   \n",
       "6  https://openalex.org/A5069881493  https://openalex.org/A5076219710   \n",
       "7  https://openalex.org/A5006873642  https://openalex.org/A5076219710   \n",
       "8  https://openalex.org/A5076219710  https://openalex.org/A5110165399   \n",
       "9  https://openalex.org/A5035579644  https://openalex.org/A5069881493   \n",
       "\n",
       "   count_coauthors  common_coauthors  \n",
       "0                1                 4  \n",
       "1                2                 6  \n",
       "2                2                 6  \n",
       "3                2                 6  \n",
       "4                1                 4  \n",
       "5                1                 4  \n",
       "6                1                 4  \n",
       "7                1                 4  \n",
       "8                1                 4  \n",
       "9                3                 8  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Criação do dataset apenas com pares que são coautores\n",
    "# Dataset terá: author_a, author_b, count_coauthors, common_coauthors\n",
    "\n",
    "print(\"Construindo dicionário de contagem de coautorias...\")\n",
    "coauthor_count = {}\n",
    "\n",
    "# Construir grafo de coautores para calcular autores em comum\n",
    "print(\"Construindo grafo de coautores...\")\n",
    "coauthor_graph = defaultdict(set)\n",
    "\n",
    "for work_id, group in tqdm(train_df.groupby(\"work_id\"), desc=\"Processando trabalhos\"):\n",
    "    authors = group[\"author_id\"].tolist()\n",
    "    if len(authors) > 1:\n",
    "        # Para cada par de autores no trabalho, incrementar o contador\n",
    "        for a, b in itertools.combinations(authors, 2):\n",
    "            # Usar min/max ao invés de sorted (mais rápido)\n",
    "            pair = (min(a, b), max(a, b))\n",
    "            coauthor_count[pair] = coauthor_count.get(pair, 0) + 1\n",
    "            # Construir grafo bidirecional\n",
    "            coauthor_graph[a].add(b)\n",
    "            coauthor_graph[b].add(a)\n",
    "\n",
    "print(f\"Total de pares que são coautores: {len(coauthor_count):,}\")\n",
    "\n",
    "# Criar dataset apenas com pares que são coautores\n",
    "print(\"\\nCriando dataset com pares de coautores e calculando autores em comum...\")\n",
    "rows = []\n",
    "\n",
    "for (author_a, author_b), count in tqdm(\n",
    "    coauthor_count.items(), desc=\"Processando pares\"\n",
    "):\n",
    "    # Calcular quantidade de autores em comum\n",
    "    coauthors_a = coauthor_graph.get(author_a, set())\n",
    "    coauthors_b = coauthor_graph.get(author_b, set())\n",
    "    # Interseção: autores que são coautores de ambos (excluindo o próprio par)\n",
    "    common_coauthors = (coauthors_a & coauthors_b) - {author_a, author_b}\n",
    "    common_count = len(common_coauthors)\n",
    "    \n",
    "    rows.append({\n",
    "        \"author_a\": author_a, \n",
    "        \"author_b\": author_b, \n",
    "        \"count_coauthors\": count,\n",
    "        \"common_coauthors\": common_count\n",
    "    })\n",
    "\n",
    "dataset_df = pd.DataFrame(rows)\n",
    "print(f\"\\nDataset criado! Total de linhas: {len(dataset_df):,}\")\n",
    "display(dataset_df.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

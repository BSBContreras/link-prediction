{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5794207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "from abc import ABC, abstractmethod\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sentence_transformers import SentenceTransformer \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1217be7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseRecommender(ABC):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.train_df = None\n",
    "        \n",
    "    @abstractmethod\n",
    "    def fit(self, train_df):\n",
    "        \"\"\"Treina o modelo com os dados de treino.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def recommend(self, author_id, top_n=10):\n",
    "        \"\"\"Retorna uma lista de author_ids recomendados.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f903cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopologyRecommender(BaseRecommender):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"Topology (Graph Coauthor)\")\n",
    "        self.graph = defaultdict(set)\n",
    "        self.popular_authors = []\n",
    "        \n",
    "    def fit(self, train_df):\n",
    "        self.train_df = train_df\n",
    "        print(f\"[{self.name}] Construindo grafo...\")\n",
    "        \n",
    "        # Construção do Grafo\n",
    "        for _, group in train_df.groupby('work_id'):\n",
    "            authors = group['author_id'].tolist()\n",
    "            if len(authors) > 1:\n",
    "                for u, v in itertools.permutations(authors, 2):\n",
    "                    self.graph[u].add(v)\n",
    "        \n",
    "        # Cálculo de Popularidade (para fallback)\n",
    "        popularity_counter = Counter()\n",
    "        for author, neighbors in self.graph.items():\n",
    "            popularity_counter[author] = len(neighbors)\n",
    "        self.popular_authors = [auth for auth, _ in popularity_counter.most_common()]\n",
    "        print(f\"[{self.name}] Grafo construído com {len(self.graph)} autores.\")\n",
    "\n",
    "    def recommend(self, author_id, top_n=10):\n",
    "        recommendations = []\n",
    "        current_coauthors = self.graph.get(author_id, set())\n",
    "        \n",
    "        # Lógica de Amigos em Comum (2 hops)\n",
    "        if author_id in self.graph:\n",
    "            candidates = []\n",
    "            for neighbor in current_coauthors:\n",
    "                neighbors_of_neighbor = self.graph.get(neighbor, set())\n",
    "                for candidate in neighbors_of_neighbor:\n",
    "                    if candidate != author_id and candidate not in current_coauthors:\n",
    "                        candidates.append(candidate)\n",
    "            \n",
    "            recommendations = [c[0] for c in Counter(candidates).most_common(top_n)]\n",
    "        \n",
    "        # Fallback: Populares\n",
    "        if len(recommendations) < top_n:\n",
    "            for pop in self.popular_authors:\n",
    "                if pop != author_id and pop not in recommendations and pop not in current_coauthors:\n",
    "                    recommendations.append(pop)\n",
    "                    if len(recommendations) >= top_n:\n",
    "                        break\n",
    "                        \n",
    "        return recommendations[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c191315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdealTopologyRecommender(BaseRecommender):\n",
    "    def __init__(self, base_topology_model, ground_truth):\n",
    "        super().__init__(\"Ideal Topology (Oracle)\")\n",
    "        self.base_model = base_topology_model\n",
    "        self.ground_truth = ground_truth\n",
    "        \n",
    "    def fit(self, train_df):\n",
    "        # Não precisamos treinar nada aqui, o modelo base já foi treinado.\n",
    "        pass\n",
    "        \n",
    "    def recommend(self, author_id, top_n=10):\n",
    "        # 1. Puxamos uma quantidade massiva de candidatos do modelo base.\n",
    "        # Isso garante que pegaremos todos os \"amigos de amigos\" que a topologia encontrou.\n",
    "        all_possible_candidates = self.base_model.recommend(author_id, top_n=5000)\n",
    "        \n",
    "        # 2. Consultamos o gabarito (quem o autor realmente conheceu no futuro)\n",
    "        actual_future_coauthors = self.ground_truth.get(author_id, set())\n",
    "        \n",
    "        # 3. A \"Trapaça\" do Oráculo: Filtramos quem é acerto e forçamos para o topo da lista\n",
    "        perfect_hits = [c for c in all_possible_candidates if c in actual_future_coauthors]\n",
    "        the_rest = [c for c in all_possible_candidates if c not in actual_future_coauthors]\n",
    "        \n",
    "        # 4. Juntamos as listas. Os acertos (hits) virão primeiro.\n",
    "        ideal_recommendation = perfect_hits + the_rest\n",
    "        \n",
    "        # Retornamos apenas a quantidade pedida pela função de avaliação\n",
    "        return ideal_recommendation[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48103f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ndcg(recommended_list, relevant_set, k):\n",
    "    \"\"\"\n",
    "    Calcula o NDCG@k (Normalized Discounted Cumulative Gain).\n",
    "    \n",
    "    Args:\n",
    "        recommended_list: Lista de itens recomendados (ordenados)\n",
    "        relevant_set: Conjunto de itens relevantes (ground truth)\n",
    "        k: Número de itens a considerar (top-k)\n",
    "    \n",
    "    Returns:\n",
    "        NDCG@k score (0.0 a 1.0)\n",
    "    \"\"\"\n",
    "    if len(relevant_set) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # DCG: Discounted Cumulative Gain\n",
    "    dcg = 0.0\n",
    "    for i, item in enumerate(recommended_list[:k], 1):\n",
    "        if item in relevant_set:\n",
    "            dcg += 1.0 / np.log2(i + 1)\n",
    "    \n",
    "    # IDCG: Ideal DCG (ordenando os relevantes primeiro)\n",
    "    idcg = 0.0\n",
    "    num_relevant = min(len(relevant_set), k)\n",
    "    for i in range(1, num_relevant + 1):\n",
    "        idcg += 1.0 / np.log2(i + 1)\n",
    "    \n",
    "    # NDCG = DCG / IDCG\n",
    "    if idcg == 0:\n",
    "        return 0.0\n",
    "    return dcg / idcg\n",
    "\n",
    "\n",
    "def calculate_mrr_at_k(recommended_list, relevant_set, k):\n",
    "    \"\"\"\n",
    "    Calcula o MRR@k (Mean Reciprocal Rank at k).\n",
    "    \n",
    "    O MRR@k só considera o primeiro item relevante se ele aparecer dentro das \n",
    "    primeiras k posições. Se o primeiro relevante aparecer após a posição k, \n",
    "    o MRR@k é 0.\n",
    "    \n",
    "    Args:\n",
    "        recommended_list: Lista de itens recomendados (ordenados)\n",
    "        relevant_set: Conjunto de itens relevantes (ground truth)\n",
    "        k: Número máximo de posições a considerar (top-k)\n",
    "    \n",
    "    Returns:\n",
    "        Reciprocal Rank@k (1/rank do primeiro relevante dentro de k, ou 0 se nenhum relevante)\n",
    "    \"\"\"\n",
    "    if len(relevant_set) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Considera apenas as primeiras k posições\n",
    "    for rank, item in enumerate(recommended_list[:k], 1):\n",
    "        if item in relevant_set:\n",
    "            return 1.0 / rank\n",
    "    \n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def evaluate_models(models, test_ground_truth, train_graph_check, K_values=[5, 10]):\n",
    "    results = {}\n",
    "    \n",
    "    # 1. Definir o maior K necessário para a avaliação\n",
    "    max_k_eval = max(K_values)\n",
    "    \n",
    "    # 2. Definir um \"Buffer\" de busca. \n",
    "    # Pedimos 2x ou 3x mais itens para garantir que, após remover \n",
    "    # os coautores antigos, ainda sobrem itens suficientes para encher o max_k_eval.\n",
    "    search_limit = max_k_eval * 3 \n",
    "    \n",
    "    for model in models:\n",
    "        print(f\"\\nAvaliando modelo: {model.name}...\")\n",
    "        model_metrics = {k: {'precision': [], 'recall': [], 'ndcg': [], 'mrr': []} for k in K_values}\n",
    "        \n",
    "        for author_id, actual_new_coauthors in test_ground_truth.items():\n",
    "            # SOLICITAÇÃO EXPANDIDA: Pede mais itens do que o necessário\n",
    "            recs = model.recommend(author_id, top_n=search_limit)\n",
    "            \n",
    "            # FILTRAGEM: Remove coautores que já existem no treino\n",
    "            past_coauthors = train_graph_check.get(author_id, set())\n",
    "            valid_recs = [r for r in recs if r not in past_coauthors]\n",
    "            \n",
    "            # CORTE FINAL: Garante que a lista tenha no máximo o tamanho do maior K avaliado\n",
    "            # Isso garante consistência: estamos avaliando as melhores \"valid_recs\" disponíveis\n",
    "            valid_recs = valid_recs[:max_k_eval]\n",
    "            \n",
    "            for k in K_values:\n",
    "                # O top_k aqui é seguro, pois valid_recs já está limpo\n",
    "                top_k_recs = valid_recs[:k]\n",
    "                \n",
    "                hits = len(set(top_k_recs) & actual_new_coauthors)\n",
    "                \n",
    "                # Métricas padrão\n",
    "                p = hits / k if k > 0 else 0\n",
    "                r = hits / len(actual_new_coauthors) if len(actual_new_coauthors) > 0 else 0\n",
    "                ndcg = calculate_ndcg(top_k_recs, actual_new_coauthors, k)\n",
    "                mrr = calculate_mrr_at_k(valid_recs, actual_new_coauthors, k) # MRR usa a lista até K implícito\n",
    "                \n",
    "                model_metrics[k]['precision'].append(p)\n",
    "                model_metrics[k]['recall'].append(r)\n",
    "                model_metrics[k]['ndcg'].append(ndcg)\n",
    "                model_metrics[k]['mrr'].append(mrr)\n",
    "        \n",
    "        # Consolidação dos resultados (Média)\n",
    "        results[model.name] = {}\n",
    "        for k in K_values:\n",
    "            avg_p = np.mean(model_metrics[k]['precision'])\n",
    "            avg_r = np.mean(model_metrics[k]['recall'])\n",
    "            avg_ndcg = np.mean(model_metrics[k]['ndcg'])\n",
    "            avg_mrr = np.mean(model_metrics[k]['mrr'])\n",
    "            f1 = 2 * (avg_p * avg_r) / (avg_p + avg_r) if (avg_p + avg_r) > 0 else 0\n",
    "            \n",
    "            results[model.name][k] = {'P': avg_p, 'R': avg_r, 'F1': f1, 'NDCG': avg_ndcg, 'MRR': avg_mrr}\n",
    "            print(f\"  K={k}: Precision={avg_p:.4f}, Recall={avg_r:.4f}, F1={f1:.4f}, NDCG={avg_ndcg:.4f}, MRR@k={avg_mrr:.4f}\")\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aefd7f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_comparison(results, figsize=(25, 5)):\n",
    "    model_names = list(results.keys())\n",
    "    k_values = sorted([k for k in results[model_names[0]].keys() if isinstance(k, int)])\n",
    "    \n",
    "    # Preparar dados para cada métrica\n",
    "    metrics_data = {\n",
    "        'Precision': {model: [results[model][k]['P'] for k in k_values] for model in model_names},\n",
    "        'Recall': {model: [results[model][k]['R'] for k in k_values] for model in model_names},\n",
    "        'F1-Score': {model: [results[model][k]['F1'] for k in k_values] for model in model_names},\n",
    "        'NDCG': {model: [results[model][k]['NDCG'] for k in k_values] for model in model_names},\n",
    "        'MRR@k': {model: [results[model][k]['MRR'] for k in k_values] for model in model_names}\n",
    "    }\n",
    "    \n",
    "    # Criar figura com 5 subplots para métricas dependentes de K\n",
    "    fig, axes = plt.subplots(1, 5, figsize=figsize)\n",
    "    fig.suptitle('Comparação de Modelos de Link Prediction', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Cores e estilos para cada modelo\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "    markers = ['o', 's', '^', 'D', 'v']\n",
    "    \n",
    "    # Plotar cada métrica dependente de K\n",
    "    for idx, (metric_name, data) in enumerate(metrics_data.items()):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        for i, model in enumerate(model_names):\n",
    "            ax.plot(\n",
    "                k_values, \n",
    "                data[model], \n",
    "                marker=markers[i % len(markers)],\n",
    "                label=model,\n",
    "                color=colors[i % len(colors)],\n",
    "                linewidth=2,\n",
    "                markersize=8\n",
    "            )\n",
    "        \n",
    "        ax.set_xlabel('K (Top-K)', fontsize=11)\n",
    "        ax.set_ylabel(metric_name, fontsize=11)\n",
    "        ax.set_title(f'{metric_name} por K', fontsize=12, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3, linestyle='--')\n",
    "        ax.legend(fontsize=9)\n",
    "        ax.set_xticks(k_values)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d01cf12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BSBCo\\AppData\\Local\\Temp\\ipykernel_24692\\1540647326.py:3: DtypeWarning: Columns (0: is_retracted) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  works_df = pd.read_csv(f'{database_path}/works.csv')\n"
     ]
    }
   ],
   "source": [
    "database_path = 'database_50k'\n",
    "authors_df = pd.read_csv(f'{database_path}/authorships.csv')\n",
    "works_df = pd.read_csv(f'{database_path}/works.csv')\n",
    "\n",
    "merged_df = authors_df.merge(\n",
    "    works_df[['id', 'publication_date', 'title', 'abstract', 'language']], \n",
    "    left_on='work_id', right_on='id'\n",
    ")\n",
    "merged_df['publication_date'] = pd.to_datetime(merged_df['publication_date'], errors='coerce')\n",
    "merged_df = merged_df.dropna(subset=['publication_date', 'author_id', 'title', 'abstract', 'language']).drop(columns=['id'])\n",
    "merged_df = merged_df[merged_df['language'] == 'en']\n",
    "\n",
    "unique_works = merged_df[['work_id', 'publication_date']].drop_duplicates().sort_values('publication_date')\n",
    "split_idx = int(len(unique_works) * 0.8)\n",
    "\n",
    "train_work_ids = set(unique_works.iloc[:split_idx]['work_id'])\n",
    "test_work_ids = set(unique_works.iloc[split_idx:]['work_id'])\n",
    "\n",
    "train_df = merged_df[merged_df['work_id'].isin(train_work_ids)]\n",
    "test_df = merged_df[merged_df['work_id'].isin(test_work_ids)]\n",
    "\n",
    "def build_graph(df):\n",
    "    graph = defaultdict(set)\n",
    "    for _, group in df.groupby('work_id'):\n",
    "        authors = group['author_id'].tolist()\n",
    "        \n",
    "        if len(authors) > 1:\n",
    "            for u, v in itertools.permutations(authors, 2):\n",
    "                graph[u].add(v)\n",
    "\n",
    "    return graph\n",
    "\n",
    "train_graph = build_graph(train_df)\n",
    "test_graph_raw = build_graph(test_df)\n",
    "\n",
    "test_ground_truth = defaultdict(set)\n",
    "\n",
    "for author, coauthors in test_graph_raw.items():\n",
    "    # Pega quem o autor colaborou no futuro\n",
    "    future_coauthors = coauthors\n",
    "    \n",
    "    # Remove quem ele já conhecia no passado (não é predição nova)\n",
    "    past_coauthors = train_graph.get(author, set())\n",
    "    new_links = future_coauthors - past_coauthors\n",
    "    \n",
    "    if new_links:\n",
    "        test_ground_truth[author] = new_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1b084bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Topology (Graph Coauthor)] Construindo grafo...\n",
      "[Topology (Graph Coauthor)] Grafo construído com 25260 autores.\n",
      "\n",
      "Avaliando modelo: Topology (Graph Coauthor)...\n",
      "  K=5: Precision=0.0169, Recall=0.0127, F1=0.0145, NDCG=0.0200, MRR@k=0.0386\n",
      "  K=10: Precision=0.0156, Recall=0.0221, F1=0.0183, NDCG=0.0225, MRR@k=0.0452\n",
      "  K=20: Precision=0.0158, Recall=0.0437, F1=0.0232, NDCG=0.0303, MRR@k=0.0540\n",
      "  K=50: Precision=0.0111, Recall=0.0704, F1=0.0192, NDCG=0.0388, MRR@k=0.0594\n",
      "  K=100: Precision=0.0071, Recall=0.0938, F1=0.0131, NDCG=0.0446, MRR@k=0.0607\n",
      "  K=200: Precision=0.0044, Recall=0.1172, F1=0.0084, NDCG=0.0503, MRR@k=0.0615\n",
      "\n",
      "Avaliando modelo: Ideal Topology (Oracle)...\n",
      "  K=5: Precision=0.3372, Recall=0.2193, F1=0.2657, NDCG=0.4902, MRR@k=0.9065\n",
      "  K=10: Precision=0.1886, Recall=0.2245, F1=0.2050, NDCG=0.4053, MRR@k=0.9065\n",
      "  K=20: Precision=0.1024, Recall=0.2270, F1=0.1411, NDCG=0.3677, MRR@k=0.9065\n",
      "  K=50: Precision=0.0449, Recall=0.2291, F1=0.0751, NDCG=0.3485, MRR@k=0.9065\n",
      "  K=100: Precision=0.0227, Recall=0.2293, F1=0.0413, NDCG=0.3420, MRR@k=0.9065\n",
      "  K=200: Precision=0.0113, Recall=0.2293, F1=0.0216, NDCG=0.3417, MRR@k=0.9065\n"
     ]
    }
   ],
   "source": [
    "topo_model = TopologyRecommender()\n",
    "ideal_topo_model = IdealTopologyRecommender(topo_model, test_ground_truth)\n",
    "\n",
    "models = [\n",
    "    topo_model,\n",
    "    ideal_topo_model\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    model.fit(train_df) \n",
    "    \n",
    "metrics = evaluate_models(models, test_ground_truth, train_graph, K_values=[5, 10, 20, 50, 100, 200])\n",
    "# plot_model_comparison(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3658c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model_comparison(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

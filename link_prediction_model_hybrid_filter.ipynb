{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5794207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "from abc import ABC, abstractmethod\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sentence_transformers import SentenceTransformer \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1217be7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseRecommender(ABC):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.train_df = None\n",
    "        \n",
    "    @abstractmethod\n",
    "    def fit(self, train_df):\n",
    "        \"\"\"Treina o modelo com os dados de treino.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def recommend(self, author_id, top_n=10):\n",
    "        \"\"\"Retorna uma lista de author_ids recomendados.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f903cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopologyRecommender(BaseRecommender):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"Topology (Graph Coauthor)\")\n",
    "        self.graph = defaultdict(set)\n",
    "        self.popular_authors = []\n",
    "        \n",
    "    def fit(self, train_df):\n",
    "        self.train_df = train_df\n",
    "        print(f\"[{self.name}] Construindo grafo...\")\n",
    "        \n",
    "        # Construção do Grafo\n",
    "        for _, group in train_df.groupby('work_id'):\n",
    "            authors = group['author_id'].tolist()\n",
    "            if len(authors) > 1:\n",
    "                for u, v in itertools.permutations(authors, 2):\n",
    "                    self.graph[u].add(v)\n",
    "        \n",
    "        # Cálculo de Popularidade (para fallback)\n",
    "        popularity_counter = Counter()\n",
    "        for author, neighbors in self.graph.items():\n",
    "            popularity_counter[author] = len(neighbors)\n",
    "        self.popular_authors = [auth for auth, _ in popularity_counter.most_common()]\n",
    "        print(f\"[{self.name}] Grafo construído com {len(self.graph)} autores.\")\n",
    "\n",
    "    def recommend(self, author_id, top_n=10):\n",
    "        recommendations = []\n",
    "        current_coauthors = self.graph.get(author_id, set())\n",
    "        \n",
    "        # Lógica de Amigos em Comum (2 hops)\n",
    "        if author_id in self.graph:\n",
    "            candidates = []\n",
    "            for neighbor in current_coauthors:\n",
    "                neighbors_of_neighbor = self.graph.get(neighbor, set())\n",
    "                for candidate in neighbors_of_neighbor:\n",
    "                    if candidate != author_id and candidate not in current_coauthors:\n",
    "                        candidates.append(candidate)\n",
    "            \n",
    "            recommendations = [c[0] for c in Counter(candidates).most_common(top_n)]\n",
    "        \n",
    "        # Fallback: Populares\n",
    "        if len(recommendations) < top_n:\n",
    "            for pop in self.popular_authors:\n",
    "                if pop != author_id and pop not in recommendations and pop not in current_coauthors:\n",
    "                    recommendations.append(pop)\n",
    "                    if len(recommendations) >= top_n:\n",
    "                        break\n",
    "                        \n",
    "        return recommendations[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e74db96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentSciBERTRecommender(BaseRecommender):\n",
    "    def __init__(self, model_name='allenai/scibert_scivocab_uncased', cache_path='scibert_embeddings.pkl'):\n",
    "        super().__init__(\"Content-Based (SciBERT)\")\n",
    "        self.model_name = model_name\n",
    "        self.cache_path = cache_path\n",
    "        self.encoder = None\n",
    "        self.author_embeddings = {} \n",
    "        self.knn_model = None\n",
    "        self.author_ids_index = []\n",
    "        \n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print(f\"[{self.name}] Device selecionado: {self.device.upper()}\")\n",
    "        \n",
    "    def fit(self, train_df):\n",
    "        work_id_to_emb = self._load_embeddings()\n",
    "        \n",
    "        unique_works = train_df[['work_id', 'title', 'abstract']].drop_duplicates()\n",
    "        \n",
    "        if work_id_to_emb is None:\n",
    "            print(f\"[{self.name}] Gerando embeddings na {self.device.upper()}...\")\n",
    "            self.encoder = SentenceTransformer(self.model_name, device=self.device)\n",
    "            \n",
    "            unique_works['text'] = unique_works['title'] + \". \" + unique_works['abstract'].fillna('')\n",
    "            \n",
    "            work_embeddings = self.encoder.encode(\n",
    "                unique_works['text'].tolist(), \n",
    "                batch_size=32, \n",
    "                show_progress_bar=True,\n",
    "                device=self.device\n",
    "            )\n",
    "            \n",
    "            work_id_to_emb = {wid: emb for wid, emb in zip(unique_works['work_id'], work_embeddings)}\n",
    "            self._save_embeddings(work_id_to_emb)\n",
    "        else:\n",
    "            print(f\"[{self.name}] Embeddings carregados do cache.\")\n",
    "\n",
    "        print(f\"[{self.name}] Consolidadando perfis de autores...\")\n",
    "        \n",
    "        author_groups = train_df.groupby('author_id')['work_id'].apply(list)\n",
    "        \n",
    "        author_vectors = []\n",
    "        self.author_ids_index = []\n",
    "        \n",
    "        for author_id, work_ids in author_groups.items():\n",
    "            vectors = [work_id_to_emb[wid] for wid in work_ids if wid in work_id_to_emb]\n",
    "            \n",
    "            if vectors:\n",
    "                mean_vector = np.mean(vectors, axis=0)\n",
    "                author_vectors.append(mean_vector)\n",
    "                self.author_ids_index.append(author_id)\n",
    "        \n",
    "        self.author_embeddings = np.array(author_vectors)\n",
    "        \n",
    "        # O KNN do scikit-learn roda em CPU, mas é muito rápido para vetores já prontos\n",
    "        self.knn_model = NearestNeighbors(n_neighbors=50, metric='cosine', n_jobs=-1)\n",
    "        self.knn_model.fit(self.author_embeddings)\n",
    "        print(f\"[{self.name}] Treino finalizado. {len(self.author_ids_index)} perfis.\")\n",
    "\n",
    "    def recommend(self, author_id, top_n=10):\n",
    "        try:\n",
    "            author_idx = self.author_ids_index.index(author_id)\n",
    "        except ValueError:\n",
    "            return []\n",
    "            \n",
    "        author_vector = self.author_embeddings[author_idx].reshape(1, -1)\n",
    "        distances, indices = self.knn_model.kneighbors(author_vector, n_neighbors=top_n+1)\n",
    "        \n",
    "        recommendations = []\n",
    "        for idx in indices[0]:\n",
    "            rec_author = self.author_ids_index[idx]\n",
    "            if rec_author != author_id:\n",
    "                recommendations.append(rec_author)\n",
    "                \n",
    "        return recommendations[:top_n]\n",
    "    \n",
    "    def get_author_vector(self, author_id):\n",
    "        try:\n",
    "            idx = self.author_ids_index.index(author_id)\n",
    "            return self.author_embeddings[idx]\n",
    "        except ValueError:\n",
    "            return None\n",
    "\n",
    "    def _save_embeddings(self, data):\n",
    "        with open(self.cache_path, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "\n",
    "    def _load_embeddings(self):\n",
    "        if os.path.exists(self.cache_path):\n",
    "            with open(self.cache_path, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4a0b612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class HybridRecommenderFilter(BaseRecommender):\n",
    "    def __init__(self, topology_model, content_model):\n",
    "        \"\"\"\n",
    "        busca k candidatos da topologia e os reordena pelo modelo de conteúdo.\n",
    "        \"\"\"\n",
    "        super().__init__(\"Hybrid (Topo Filter + Content Rank)\")\n",
    "        self.topo_model = topology_model\n",
    "        self.content_model = content_model\n",
    "        \n",
    "    def fit(self, train_df):\n",
    "        pass\n",
    "\n",
    "    def recommend(self, author_id, top_n=10):\n",
    "        candidates = self.topo_model.recommend(author_id, top_n=top_n)\n",
    "        \n",
    "        if not candidates:\n",
    "            return []\n",
    "            \n",
    "        source_vector = self.content_model.get_author_vector(author_id)\n",
    "        \n",
    "        if source_vector is None:\n",
    "            return candidates[:top_n]\n",
    "            \n",
    "        candidate_scores = []\n",
    "        \n",
    "        for cand_id in candidates:\n",
    "            cand_vector = self.content_model.get_author_vector(cand_id)\n",
    "            \n",
    "            if cand_vector is not None:\n",
    "                sim = cosine_similarity(source_vector.reshape(1, -1), cand_vector.reshape(1, -1))[0][0]\n",
    "                candidate_scores.append((cand_id, sim))\n",
    "            else:\n",
    "                candidate_scores.append((cand_id, -1))\n",
    "        \n",
    "        candidate_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        final_recs = [cand for cand, score in candidate_scores]\n",
    "        return final_recs[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48103f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ndcg(recommended_list, relevant_set, k):\n",
    "    \"\"\"\n",
    "    Calcula o NDCG@k (Normalized Discounted Cumulative Gain).\n",
    "    \n",
    "    Args:\n",
    "        recommended_list: Lista de itens recomendados (ordenados)\n",
    "        relevant_set: Conjunto de itens relevantes (ground truth)\n",
    "        k: Número de itens a considerar (top-k)\n",
    "    \n",
    "    Returns:\n",
    "        NDCG@k score (0.0 a 1.0)\n",
    "    \"\"\"\n",
    "    if len(relevant_set) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # DCG: Discounted Cumulative Gain\n",
    "    dcg = 0.0\n",
    "    for i, item in enumerate(recommended_list[:k], 1):\n",
    "        if item in relevant_set:\n",
    "            dcg += 1.0 / np.log2(i + 1)\n",
    "    \n",
    "    # IDCG: Ideal DCG (ordenando os relevantes primeiro)\n",
    "    idcg = 0.0\n",
    "    num_relevant = min(len(relevant_set), k)\n",
    "    for i in range(1, num_relevant + 1):\n",
    "        idcg += 1.0 / np.log2(i + 1)\n",
    "    \n",
    "    # NDCG = DCG / IDCG\n",
    "    if idcg == 0:\n",
    "        return 0.0\n",
    "    return dcg / idcg\n",
    "\n",
    "\n",
    "def calculate_mrr_at_k(recommended_list, relevant_set, k):\n",
    "    \"\"\"\n",
    "    Calcula o MRR@k (Mean Reciprocal Rank at k).\n",
    "    \n",
    "    O MRR@k só considera o primeiro item relevante se ele aparecer dentro das \n",
    "    primeiras k posições. Se o primeiro relevante aparecer após a posição k, \n",
    "    o MRR@k é 0.\n",
    "    \n",
    "    Args:\n",
    "        recommended_list: Lista de itens recomendados (ordenados)\n",
    "        relevant_set: Conjunto de itens relevantes (ground truth)\n",
    "        k: Número máximo de posições a considerar (top-k)\n",
    "    \n",
    "    Returns:\n",
    "        Reciprocal Rank@k (1/rank do primeiro relevante dentro de k, ou 0 se nenhum relevante)\n",
    "    \"\"\"\n",
    "    if len(relevant_set) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Considera apenas as primeiras k posições\n",
    "    for rank, item in enumerate(recommended_list[:k], 1):\n",
    "        if item in relevant_set:\n",
    "            return 1.0 / rank\n",
    "    \n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def evaluate_models(models, test_ground_truth, train_graph_check, K_values=[5, 10]):\n",
    "    results = {}\n",
    "    \n",
    "    max_k_eval = max(K_values)\n",
    "    search_limit = max_k_eval \n",
    "    \n",
    "    for model in models:\n",
    "        print(f\"\\nAvaliando modelo: {model.name}...\")\n",
    "        model_metrics = {k: {'precision': [], 'recall': [], 'ndcg': [], 'mrr': []} for k in K_values}\n",
    "        \n",
    "        for author_id, actual_new_coauthors in test_ground_truth.items():\n",
    "            # Ambos os modelos recebem o limite estrito (ex: 50)\n",
    "            recs = model.recommend(author_id, top_n=search_limit)\n",
    "            \n",
    "            # FILTRAGEM\n",
    "            past_coauthors = train_graph_check.get(author_id, set())\n",
    "            valid_recs = [r for r in recs if r not in past_coauthors]\n",
    "            \n",
    "            for k in K_values:\n",
    "                # Se a lista ficou menor que K (ex: pediu 50, 5 eram velhos, sobrou 45),\n",
    "                # avaliamos os 45. Como isso acontece igual para o Híbrido e Topologia,\n",
    "                # as métricas globais serão idênticas.\n",
    "                top_k_recs = valid_recs[:k]\n",
    "                \n",
    "                hits = len(set(top_k_recs) & actual_new_coauthors)\n",
    "                \n",
    "                # Métricas\n",
    "                p = hits / k if k > 0 else 0\n",
    "                r = hits / len(actual_new_coauthors) if len(actual_new_coauthors) > 0 else 0\n",
    "                ndcg = calculate_ndcg(top_k_recs, actual_new_coauthors, k)\n",
    "                mrr = calculate_mrr_at_k(valid_recs, actual_new_coauthors, k)\n",
    "                \n",
    "                model_metrics[k]['precision'].append(p)\n",
    "                model_metrics[k]['recall'].append(r)\n",
    "                model_metrics[k]['ndcg'].append(ndcg)\n",
    "                model_metrics[k]['mrr'].append(mrr)\n",
    "        \n",
    "        # Consolidação\n",
    "        results[model.name] = {}\n",
    "        for k in K_values:\n",
    "            avg_p = np.mean(model_metrics[k]['precision'])\n",
    "            avg_r = np.mean(model_metrics[k]['recall'])\n",
    "            avg_ndcg = np.mean(model_metrics[k]['ndcg'])\n",
    "            avg_mrr = np.mean(model_metrics[k]['mrr'])\n",
    "            f1 = 2 * (avg_p * avg_r) / (avg_p + avg_r) if (avg_p + avg_r) > 0 else 0\n",
    "            \n",
    "            results[model.name][k] = {'P': avg_p, 'R': avg_r, 'F1': f1, 'NDCG': avg_ndcg, 'MRR': avg_mrr}\n",
    "            print(f\"  K={k}: Precision={avg_p:.4f}, Recall={avg_r:.4f}, F1={f1:.4f}, NDCG={avg_ndcg:.4f}, MRR@k={avg_mrr:.4f}\")\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aefd7f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_comparison(results, figsize=(25, 5)):\n",
    "    model_names = list(results.keys())\n",
    "    k_values = sorted([k for k in results[model_names[0]].keys() if isinstance(k, int)])\n",
    "    \n",
    "    # Preparar dados para cada métrica\n",
    "    metrics_data = {\n",
    "        'Precision': {model: [results[model][k]['P'] for k in k_values] for model in model_names},\n",
    "        'Recall': {model: [results[model][k]['R'] for k in k_values] for model in model_names},\n",
    "        'F1-Score': {model: [results[model][k]['F1'] for k in k_values] for model in model_names},\n",
    "        'NDCG': {model: [results[model][k]['NDCG'] for k in k_values] for model in model_names},\n",
    "        'MRR@k': {model: [results[model][k]['MRR'] for k in k_values] for model in model_names}\n",
    "    }\n",
    "    \n",
    "    # Criar figura com 5 subplots para métricas dependentes de K\n",
    "    fig, axes = plt.subplots(1, 5, figsize=figsize)\n",
    "    fig.suptitle('Comparação de Modelos de Link Prediction', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Cores e estilos para cada modelo\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "    markers = ['o', 's', '^', 'D', 'v']\n",
    "    \n",
    "    # Plotar cada métrica dependente de K\n",
    "    for idx, (metric_name, data) in enumerate(metrics_data.items()):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        for i, model in enumerate(model_names):\n",
    "            ax.plot(\n",
    "                k_values, \n",
    "                data[model], \n",
    "                marker=markers[i % len(markers)],\n",
    "                label=model,\n",
    "                color=colors[i % len(colors)],\n",
    "                linewidth=2,\n",
    "                markersize=8\n",
    "            )\n",
    "        \n",
    "        ax.set_xlabel('K (Top-K)', fontsize=11)\n",
    "        ax.set_ylabel(metric_name, fontsize=11)\n",
    "        ax.set_title(f'{metric_name} por K', fontsize=12, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3, linestyle='--')\n",
    "        ax.legend(fontsize=9)\n",
    "        ax.set_xticks(k_values)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d01cf12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BSBCo\\AppData\\Local\\Temp\\ipykernel_7692\\1540647326.py:3: DtypeWarning: Columns (0: is_retracted) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  works_df = pd.read_csv(f'{database_path}/works.csv')\n"
     ]
    }
   ],
   "source": [
    "database_path = 'database_50k'\n",
    "authors_df = pd.read_csv(f'{database_path}/authorships.csv')\n",
    "works_df = pd.read_csv(f'{database_path}/works.csv')\n",
    "\n",
    "merged_df = authors_df.merge(\n",
    "    works_df[['id', 'publication_date', 'title', 'abstract', 'language']], \n",
    "    left_on='work_id', right_on='id'\n",
    ")\n",
    "merged_df['publication_date'] = pd.to_datetime(merged_df['publication_date'], errors='coerce')\n",
    "merged_df = merged_df.dropna(subset=['publication_date', 'author_id', 'title', 'abstract', 'language']).drop(columns=['id'])\n",
    "merged_df = merged_df[merged_df['language'] == 'en']\n",
    "\n",
    "unique_works = merged_df[['work_id', 'publication_date']].drop_duplicates().sort_values('publication_date')\n",
    "split_idx = int(len(unique_works) * 0.8)\n",
    "\n",
    "train_work_ids = set(unique_works.iloc[:split_idx]['work_id'])\n",
    "test_work_ids = set(unique_works.iloc[split_idx:]['work_id'])\n",
    "\n",
    "train_df = merged_df[merged_df['work_id'].isin(train_work_ids)]\n",
    "test_df = merged_df[merged_df['work_id'].isin(test_work_ids)]\n",
    "\n",
    "def build_graph(df):\n",
    "    graph = defaultdict(set)\n",
    "    for _, group in df.groupby('work_id'):\n",
    "        authors = group['author_id'].tolist()\n",
    "        \n",
    "        if len(authors) > 1:\n",
    "            for u, v in itertools.permutations(authors, 2):\n",
    "                graph[u].add(v)\n",
    "\n",
    "    return graph\n",
    "\n",
    "train_graph = build_graph(train_df)\n",
    "test_graph_raw = build_graph(test_df)\n",
    "\n",
    "test_ground_truth = defaultdict(set)\n",
    "\n",
    "for author, coauthors in test_graph_raw.items():\n",
    "    # Pega quem o autor colaborou no futuro\n",
    "    future_coauthors = coauthors\n",
    "    \n",
    "    # Remove quem ele já conhecia no passado (não é predição nova)\n",
    "    past_coauthors = train_graph.get(author, set())\n",
    "    new_links = future_coauthors - past_coauthors\n",
    "    \n",
    "    if new_links:\n",
    "        test_ground_truth[author] = new_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1b084bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Content-Based (SciBERT)] Device selecionado: CUDA\n",
      "[Topology (Graph Coauthor)] Construindo grafo...\n",
      "[Topology (Graph Coauthor)] Grafo construído com 25260 autores.\n",
      "[Content-Based (SciBERT)] Embeddings carregados do cache.\n",
      "[Content-Based (SciBERT)] Consolidadando perfis de autores...\n",
      "[Content-Based (SciBERT)] Treino finalizado. 25262 perfis.\n",
      "\n",
      "Avaliando modelo: Topology (Graph Coauthor)...\n",
      "  K=5: Precision=0.0165, Recall=0.0127, F1=0.0144, NDCG=0.0194, MRR@k=0.0371\n",
      "  K=10: Precision=0.0149, Recall=0.0220, F1=0.0178, NDCG=0.0218, MRR@k=0.0437\n",
      "  K=20: Precision=0.0156, Recall=0.0433, F1=0.0229, NDCG=0.0299, MRR@k=0.0525\n",
      "  K=50: Precision=0.0111, Recall=0.0706, F1=0.0191, NDCG=0.0387, MRR@k=0.0579\n",
      "\n",
      "Avaliando modelo: Content-Based (SciBERT)...\n",
      "  K=5: Precision=0.0057, Recall=0.0034, F1=0.0043, NDCG=0.0067, MRR@k=0.0125\n",
      "  K=10: Precision=0.0048, Recall=0.0054, F1=0.0051, NDCG=0.0068, MRR@k=0.0139\n",
      "  K=20: Precision=0.0039, Recall=0.0083, F1=0.0053, NDCG=0.0074, MRR@k=0.0148\n",
      "  K=50: Precision=0.0023, Recall=0.0119, F1=0.0038, NDCG=0.0080, MRR@k=0.0152\n",
      "\n",
      "Avaliando modelo: Hybrid (Topo Filter + Content Rank)...\n",
      "  K=5: Precision=0.0167, Recall=0.0124, F1=0.0142, NDCG=0.0185, MRR@k=0.0348\n",
      "  K=10: Precision=0.0159, Recall=0.0221, F1=0.0185, NDCG=0.0217, MRR@k=0.0419\n",
      "  K=20: Precision=0.0161, Recall=0.0443, F1=0.0236, NDCG=0.0298, MRR@k=0.0507\n",
      "  K=50: Precision=0.0111, Recall=0.0706, F1=0.0191, NDCG=0.0380, MRR@k=0.0560\n"
     ]
    }
   ],
   "source": [
    "topo_model = TopologyRecommender()\n",
    "content_model = ContentSciBERTRecommender(model_name='allenai/scibert_scivocab_uncased', cache_path=f'{database_path}/scibert_embeddings.pkl')\n",
    "hybrid_model_filter = HybridRecommenderFilter(topo_model, content_model)\n",
    "\n",
    "models = [\n",
    "    topo_model, \n",
    "    content_model, \n",
    "    hybrid_model_filter\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    model.fit(train_df) \n",
    "    \n",
    "metrics = evaluate_models(models, test_ground_truth, train_graph, K_values=[5, 10, 20, 50])\n",
    "# plot_model_comparison(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
